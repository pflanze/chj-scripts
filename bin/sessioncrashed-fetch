#!/usr/bin/perl -w

# Son Feb 10 17:36:24 CET 2008
(my $email='pflanze%gmx,ch')=~ tr/%,/@./;

use strict;
use Chj::FP::lazy;
use Chj::xpipeline 'xxpipeline';

our $sessioncrashedsourcefile_default= "$ENV{HOME}/.galeon/session_crashed.xml";
sub chompingbacktick ( $ ) {
    my $res= `$_[0]`;
    chomp $res;
    $res
}

our $sessioncrashedfile_copy= "urls_".chompingbacktick("date_").".xml";
our $sessioncrashedfile_copy_tmp= $sessioncrashedfile_copy."~";
our $Sessioncrashedfile_log= Delay {
    my $s= $sessioncrashedfile_copy;
    $s=~ s/\.xml\z//;
    $s.".log";
};

$0=~ /(.*?)([^\/]+)\z/s or die "?";
my ($mydir, $myname)=($1,$2);
sub usage {
    print STDERR map{"$_\n"} @_ if @_;
    print "$myname

  Fetch all pages you've got currently open in galeon.
  Please close windows/frames you don't want to be fetched, first.

  Options:
  --from sessioncrashedfile  ('$sessioncrashedsourcefile_default' by default)
  --to  copy_as_url_file  ('$sessioncrashedfile_copy' by default)
         (the wget-page logfile will be created from this name
          with the (optional) .xml suffix stripped and .log appended)

  --dry-run   do only display the urls
  --force     do not ask for confirmation before downloading

  (Christian Jaeger <$email>)
";
exit (@_ ? 1 : 0);
}

use Getopt::Long;
our $verbose=0;
our $dryrun=0;
our $force=0;
our $sessioncrashedsourcefile= $sessioncrashedsourcefile_default;
GetOptions("verbose"=> \$verbose,
	   "help"=> sub{usage},
	   "dry-run"=> \$dryrun,
	   "force"=> \$force,
	   "from=s"=> \$sessioncrashedsourcefile, #nicht from|s  duh.
	   ) or exit 1;
usage if @ARGV;

use Chj::xperlfunc;

xxsystem("cp",
	 #"-a", no. current mtime, ok?
	 $sessioncrashedsourcefile,
	 $sessioncrashedfile_copy_tmp);

use XML::LibXML;
our $parser= XML::LibXML->new; # (notes about XML::LibXML{,::Parser} oddity stripped)

$parser->line_numbers(1);

our $doc = $parser->parse_file($sessioncrashedfile_copy_tmp)
  or die "could not parse file '$sessioncrashedfile_copy_tmp' (os error?: $!)";

use URI;
sub url_canonicalize {
    my ($str)=@_;
    my $uri= URI->new($str);
    $uri->canonical->as_string
}
#use Chj::Backtrace; use Chj::repl; repl;
#__END__
our $errors=0;
our @urls=
  do {
      my $seen={};
      grep {
	  (m|^https?://|is ?
	   do {
	       my $canonical= url_canonicalize ($_);
	       (exists $$seen{$canonical} ?
		do {
		    warn "$myname: ignoring url '$_' because it's a double\n";
		    0
		}
		:
		do {
		    $$seen{$canonical}=undef;
		    1
		})
	   }
	   :
	   do {
	       warn "$myname: url '$_' does not look like something we can fetch\n";
	       $errors++;
	       0
	   })
      }
      map { $_ ->getAttribute("url") }
      $doc ->findnodes ("/session/window/embed");
  };

use Chj::Util::AskYN;
sub make_sure ( $ ) {
    my ($question)=@_;
    if (!maybe_askyn($question)) { #should rather be called ask_confirmation or something like that.
	die "$myname: exiting on user request.\n";
    }
}

our $prin= sub{
    print map {"  $_\n"} @urls
      or die $!;
};
if ($dryrun) {
    &$prin
} else {
    if ($force) {
	# just do it
    } else {
	&$prin;
	make_sure("Do you want to download the above ".@urls." url's?");
    }
    my $logfile= Force $Sessioncrashedfile_log;
    my $logfile_tmp= $logfile.".tmp";
    die "logfile temp file '$logfile_tmp' already exists"
      if -e $logfile_tmp; #(well. we don't check the other one. but w'ever)
    xxpipeline( sub {
		    open STDERR,">&STDOUT" or die $!;
		    xexec "wget-page", @urls
		},
		[ "tee", "--append", "--ignore-interrupts", $logfile_tmp ] # append just for safety.?.
	      );
    xrename $logfile_tmp, $logfile;
    xrename $sessioncrashedfile_copy_tmp, $sessioncrashedfile_copy;
    print "$myname: done.\n";
}
